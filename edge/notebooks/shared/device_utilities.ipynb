{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "device_requirements": {
            "min_memory": "1GB",
            "gpu_required": false,
            "min_cuda_capability": "0.0",
            "recommended_device_type": ["jetson-agx-orin", "jetson-nano", "raspberry-pi"],
            "max_batch_size": 1
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Device Utilities and Common Functions\n",
                "\n",
                "This notebook contains shared utilities and helper functions that are device-agnostic.\n",
                "These utilities can be imported and used by other notebooks regardless of the device type.\n",
                "\n",
                "## Contents\n",
                "1. Device Capability Checking\n",
                "2. Resource Monitoring\n",
                "3. Image Processing Utilities\n",
                "4. Model Management\n",
                "5. Performance Profiling"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import sys\n",
                "sys.path.append('../..')\n",
                "from base_config import DeviceConfig\n",
                "from monitoring import DeviceMonitor\n",
                "import json\n",
                "import time\n",
                "from pathlib import Path\n",
                "import cv2\n",
                "import numpy as np\n",
                "\n",
                "class DeviceUtils:\n",
                "    def __init__(self):\n",
                "        self.config = DeviceConfig()\n",
                "        self.monitor = DeviceMonitor()\n",
                "    \n",
                "    def check_compatibility(self, notebook_path: str) -> bool:\n",
                "        \"\"\"Check if a notebook is compatible with current device.\"\"\"\n",
                "        with open(notebook_path, 'r') as f:\n",
                "            notebook = json.load(f)\n",
                "            if 'device_requirements' in notebook.get('metadata', {}):\n",
                "                return self.config.check_notebook_compatibility(\n",
                "                    notebook['metadata']['device_requirements']\n",
                "                )\n",
                "        return True  # No requirements specified\n",
                "\n",
                "    def profile_execution(self, func, *args, **kwargs):\n",
                "        \"\"\"Profile function execution with resource monitoring.\"\"\"\n",
                "        start_metrics = self.monitor.collect_metrics()\n",
                "        start_time = time.time()\n",
                "        \n",
                "        result = func(*args, **kwargs)\n",
                "        \n",
                "        end_time = time.time()\n",
                "        end_metrics = self.monitor.collect_metrics()\n",
                "        \n",
                "        return {\n",
                "            'result': result,\n",
                "            'execution_time': end_time - start_time,\n",
                "            'memory_delta': end_metrics['memory']['used'] - start_metrics['memory']['used'],\n",
                "            'cpu_usage': end_metrics['cpu']['usage_percent']\n",
                "        }\n",
                "\n",
                "utils = DeviceUtils()\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class ImageUtils:\n",
                "    @staticmethod\n",
                "    def optimize_image_size(image, target_size, max_memory_mb=100):\n",
                "        \"\"\"Optimize image size based on memory constraints.\"\"\"\n",
                "        current_memory = image.nbytes / (1024 * 1024)  # Convert to MB\n",
                "        if current_memory > max_memory_mb:\n",
                "            scale_factor = np.sqrt(max_memory_mb / current_memory)\n",
                "            new_size = tuple(int(dim * scale_factor) for dim in target_size)\n",
                "            return cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n",
                "        return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
                "    \n",
                "    @staticmethod\n",
                "    def efficient_normalize(image):\n",
                "        \"\"\"Memory-efficient image normalization.\"\"\"\n",
                "        image = image.astype(np.float32)\n",
                "        cv2.normalize(image, image, 0, 1, cv2.NORM_MINMAX)\n",
                "        return image\n",
                "    \n",
                "    @staticmethod\n",
                "    def create_processing_pipeline(operations):\n",
                "        \"\"\"Create a memory-efficient image processing pipeline.\"\"\"\n",
                "        def pipeline(image):\n",
                "            for operation in operations:\n",
                "                image = operation(image)\n",
                "                if image is None:\n",
                "                    raise ValueError(f\"Operation {operation.__name__} failed\")\n",
                "            return image\n",
                "        return pipeline\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class ModelUtils:\n",
                "    @staticmethod\n",
                "    def get_model_size(model_path):\n",
                "        \"\"\"Get model file size and memory requirements.\"\"\"\n",
                "        return Path(model_path).stat().st_size\n",
                "    \n",
                "    @staticmethod\n",
                "    def estimate_memory_requirement(model_path, batch_size=1):\n",
                "        \"\"\"Estimate memory requirements for model inference.\"\"\"\n",
                "        model_size = ModelUtils.get_model_size(model_path)\n",
                "        # Rough estimation: model size * 2 for runtime + batch_size factor\n",
                "        return model_size * 2 * batch_size\n",
                "    \n",
                "    @staticmethod\n",
                "    def check_model_compatibility(model_path):\n",
                "        \"\"\"Check if model can run on current device.\"\"\"\n",
                "        config = DeviceConfig()\n",
                "        required_memory = ModelUtils.estimate_memory_requirement(model_path)\n",
                "        available_memory = config.get_resource_limits()['memory']['available']\n",
                "        return required_memory < available_memory\n"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}
